{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import more_itertools as mit\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.style.use('dark_background')\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from six.moves import cPickle\n",
    "\n",
    "import os\n",
    "\n",
    "from scipy import signal as spsig\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.signal import argrelmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_files(path, pattern=r'**/*.dat'):\n",
    "    yield from (file for file in data_dir.glob('**/*.dat'))\n",
    "    \n",
    "\n",
    "def _extract_digit(pattern, text):\n",
    "    \"\"\" helper for extract_bw_and_sf \"\"\"\n",
    "    try:\n",
    "        match = int(re.search(pattern, text)[1])\n",
    "    except:\n",
    "        print('ERROR! unable to extract bw or sf')\n",
    "        return \n",
    "    else:\n",
    "        return match \n",
    "    \n",
    "    \n",
    "def extract_bw_and_sf(filename, BW_val):\n",
    "    \"\"\" extracts params from input filename \"\"\"\n",
    "    #_bw_pattern, _sf_pattern, _l_pattern = r'BW(\\d)', r'SF(\\d{1,})', r'L(\\d{1,})'\n",
    "    _bw_pattern, _sf_pattern, _l_pattern = r'BW(\\d)', r'SF(\\d{1,})', r'Att(\\d{1,})'\n",
    "    bw_match = _extract_digit(_bw_pattern, filename)\n",
    "    \n",
    "    if bw_match is not None:\n",
    "        bw_match = BW_val[bw_match - 1]\n",
    "    \n",
    "    sf_match = _extract_digit(_sf_pattern, filename)\n",
    "    \n",
    "    l_match = _extract_digit(_l_pattern, filename)\n",
    "    \n",
    "    return bw_match, sf_match, l_match\n",
    "\n",
    "def check_and_load_file(filepath):\n",
    "    filepath = pathlib.Path(filepath)\n",
    "    \n",
    "    if not isinstance(filepath, pathlib.Path) or not filepath.exists():\n",
    "        print(f'ERROR! unable to find input file at:\\n{filepath}')\n",
    "        return\n",
    "        \n",
    "    return filepath\n",
    "\n",
    "\n",
    "def _average_packet_length():\n",
    "    sampPerSym = np.round(((2**SF)/BW)*Fs)\n",
    "    pcktLen = 30.25*sampPerSym\n",
    "    return pcktLen\n",
    "\n",
    "\n",
    "\n",
    "### get encoding parameters\n",
    "\n",
    "def get_and_set_encoding_params(filepath):\n",
    "    BW_val = np.array([1, 2, 0, 0, 0, 0, 7, 8, 9])\n",
    "    BW_val2 = np.array([0,10.4e3, 15.6e3, 0, 0, 0, 0, 125e3, 250e3, 500e3])\n",
    "    BW, SF, Att= extract_bw_and_sf(filepath.name, BW_val) \n",
    "    \n",
    "    Fs = int(1e6)\n",
    "\n",
    "    sampPerSym = np.round(((2**SF)/BW_val2[BW])*Fs)\n",
    "    pcktLen = 30.25*sampPerSym\n",
    "\n",
    "    print(f'BW: {BW} | SF: {SF} | Fs: {Fs} | Samples Per Symbol: {sampPerSym} | Packet Length: {pcktLen}')\n",
    "    return BW, SF, sampPerSym, Att\n",
    "\n",
    "\n",
    "## load data\n",
    "def load_data(filepath):\n",
    "    try:\n",
    "        signal = np.fromfile(filepath, dtype=np.complex64)\n",
    "    except Exception as exc:\n",
    "        print(f'unable to load file:\\n{exc}')\n",
    "        return \n",
    "    else:\n",
    "        #print(f'loaded signal with {signal.size} samples')\n",
    "        return signal\n",
    "\n",
    "    \n",
    "## normalize signal\n",
    "def normalize_signal(signal, th=0.001):\n",
    "    real_s = np.abs(np.real(signal)) \n",
    "    \n",
    "    norm_s = np.array([\n",
    "        np.ceil(val) if val >= th else 0\n",
    "        for val in real_s\n",
    "    ])\n",
    "    return real_s, norm_s\n",
    "\n",
    "\n",
    "### locate zero indices\n",
    "def _locate_zero_indices(norm_signal):\n",
    "    indices = np.where(norm_signal == 0.0)[0]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "### find consecutive groups \n",
    "def _find_consecutive_groups(zero_indexes):\n",
    "    groups = [\n",
    "        list(j) for j in \n",
    "        mit.consecutive_groups(sorted(list(set(zero_indexes))))\n",
    "    ]\n",
    "    \n",
    "    lengths = np.array(\n",
    "        [len(item) for item in groups]\n",
    "    )\n",
    "    \n",
    "    #print(f'found {len(groups)} groups')\n",
    "    return groups, lengths\n",
    "\n",
    "### locate endpoints\n",
    "def _locate_endpoints(groups, threshold=15_000):\n",
    "    all_endpoints = [\n",
    "        endpoints for group in groups\n",
    "            if len(group) > threshold\n",
    "        for endpoints in (group[0], group[-1])\n",
    "    ]\n",
    "    \n",
    "    endpoint_pairs = [\n",
    "        (start, stop) \n",
    "        for start, stop in zip(all_endpoints[1::2], all_endpoints[2::2])\n",
    "    ]\n",
    "    \n",
    "    #print(f'Extracted {len(endpoint_pairs)} packets from signal')\n",
    "    return endpoint_pairs\n",
    "\n",
    "## extract endpoints\n",
    "def plot_groups(indexes, lengths, filename):\n",
    "    fig, axs = plt.subplots(2)\n",
    "    fig.suptitle(f'{filename}')\n",
    "    axs[0].plot(indexes)\n",
    "    axs[1].plot(lengths)\n",
    "    \n",
    "    #plt.savefig(f'plots/{filename}.png')\n",
    "    return fig\n",
    "\n",
    "def extract_endpoints(normalized_signal, filename, threshold=15_000):\n",
    "    zero_indexes = _locate_zero_indices(normalized_signal)\n",
    "    consec_groups, groups_lengths = _find_consecutive_groups(zero_indexes)\n",
    "\n",
    "    # validation plot \n",
    "    plot_groups(zero_indexes, groups_lengths, filename)\n",
    "    \n",
    "    endpoint_pairs = _locate_endpoints(consec_groups, threshold)\n",
    "    return endpoint_pairs\n",
    "\n",
    "def extract_indices(signal, scale_factor=101, window_length=17_500):\n",
    "    real_s, down_s = preprocess(signal, scale_factor, window_length)\n",
    "    \n",
    "    packet_len = _average_packet_length()\n",
    "    minima = find_minima(down_s, window=packet_len//scale_factor)\n",
    "    \n",
    "    _plot_comparison(real_s, down_s, minima)\n",
    "    return minima \n",
    "\n",
    "\n",
    "def extract_endpoints_od(signal, scale_factor=101, window_length=17_500):\n",
    "    minima = extract_indices(signal, scale_factor, window_length)\n",
    "    \n",
    "    endpoints = _generate_endpoints(minima, scale_factor, sampPerSym)\n",
    "    return endpoints\n",
    "\n",
    "\n",
    "def find_minima(signal, window=8_000):\n",
    "    minima = spsig.argrelmin(signal, order=int(window))[0]\n",
    "    \n",
    "    return minima\n",
    "\n",
    "\n",
    "def _generate_minima_plot(minima, size):\n",
    "    minima_plot = np.zeros(size)\n",
    "    \n",
    "    for val in minima:\n",
    "        minima_plot[val] = 1 \n",
    "        \n",
    "    return minima_plot\n",
    "\n",
    "\n",
    "def _moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "\n",
    "def _plot_comparison(real, down, minima):\n",
    "    fig, axs = plt.subplots(3)\n",
    "    fig.set_figwidth(10), fig.set_figheight(12)\n",
    "    axs[0].plot(real[100_000:])\n",
    "    axs[1].plot(down[1000:])\n",
    "    axs[2].plot(_generate_minima_plot(minima, down.size)[1000:])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(signal, scale_factor, window_length=17_500):\n",
    "    real_s = np.abs(np.real(signal))\n",
    "    \n",
    "    down_s = _moving_average(\n",
    "        spsig.decimate(real_s, scale_factor, ftype='iir'), \n",
    "        n=window_length // scale_factor\n",
    "    )\n",
    "    print(f'decimated by a factor of {scale_factor} | size: {down_s.size}')\n",
    "    \n",
    "    return real_s, down_s\n",
    "    \n",
    "\n",
    "\n",
    "## packet slicing\n",
    "def _slice_and_pad(signal, endpoints, length):\n",
    "    start, stop = endpoints\n",
    "    sliced = signal[start:stop] \n",
    "    \n",
    "    if len(sliced) < length:\n",
    "        sliced = np.concatenate((\n",
    "            sliced, np.zeros(length - len(sliced))\n",
    "        ))\n",
    "        \n",
    "    return sliced\n",
    "\n",
    "\n",
    "def slice_all_packets(signal, endpoints):\n",
    "    max_length = max(stop - start for start, stop in endpoints)\n",
    "    print(f'got max packet length: {max_length}')\n",
    "    \n",
    "    packets = np.vstack(\n",
    "        tuple(\n",
    "            _slice_and_pad(signal, pair, max_length)\n",
    "            for pair in endpoints\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #print(f'Extracted {len(packets)} packets from signal')\n",
    "    return packets\n",
    "\n",
    "\n",
    "def _generate_endpoints(minima, scale, sampPerSym):\n",
    "    packet_length = 30.25*sampPerSym\n",
    "    return [\n",
    "        (int(val*scale), int(val*scale+packet_length))\n",
    "        for val in minima\n",
    "    ]\n",
    "\n",
    "def reshape_symbol_sets(d, num_samples):\n",
    "    num_symbols, sym_len  = d.shape\n",
    "    \n",
    "    if sym_len < num_samples:\n",
    "        R = int(np.ceil(num_samples/sym_len))\n",
    "        N = int(np.floor(num_symbols/R))\n",
    "    else:\n",
    "        return d\n",
    "        \n",
    "    \n",
    "    final_data = np.empty((N, num_samples), dtype='complex')\n",
    "    \n",
    "    for idx, r in enumerate(range(R, N, R)):\n",
    "        # r is the row selectors for the start of each set to be reshaped\n",
    "        last_r = last_r if r != R else 0\n",
    "        \n",
    "        data = d[last_r:r, :]\n",
    "        d1d = np.reshape(data, data.size)\n",
    "        final_data[idx, :] = d1d\n",
    "\n",
    "        last_r = r\n",
    "        \n",
    "    return final_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('Raw_Data/Benchtop/Attenuated/New/')\n",
    "all_data_files = list(glob_files(data_dir))\n",
    "\n",
    "print(\"# of files: \" + str(len(all_data_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract packets, symbols, & save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_packets(filepath, norm_thresh=0.001, packet_thresh=15_000):\n",
    "    filepath = check_and_load_file(filepath)    \n",
    "    #print(f'norm threshold: {norm_thresh}')\n",
    "    \n",
    "    get_and_set_encoding_params(filepath) \n",
    "    signal = load_data(filepath)\n",
    "    \n",
    "    #N = 3_000_000 \n",
    "    N = len(signal)\n",
    "    \n",
    "    real_s, norm_s = normalize_signal(signal[0:N], norm_thresh)\n",
    "    #print(\"Normalization Complete\")\n",
    "  \n",
    "    endpoint_pairs = extract_endpoints(norm_s, filepath.name, packet_thresh)\n",
    "    print(\"Endpoint Pairs Extracted. Total: \" + str(np.shape(endpoint_pairs)))\n",
    "    \n",
    "    x = np.shape(np.asarray(endpoint_pairs))\n",
    "    if x[0] != 0:\n",
    "        all_packets = slice_all_packets(signal[0:N], endpoint_pairs)\n",
    "        print(\"Total Packets: \" + str(np.shape(all_packets)))\n",
    "    \n",
    "        return all_packets\n",
    "    else: \n",
    "        return [0]\n",
    "\n",
    "def extract_symbols(packets, numPackets, sampPerSym):\n",
    "    \n",
    "    if sampPerSym < np.asarray(packets).shape[1]:\n",
    "        # Get Params\n",
    "        numSymbols = 8\n",
    "\n",
    "        #print(\"Total Packet:\" + str(numPackets))\n",
    "        #print(\"Symbols to Extract: \" + str(numSymbols))\n",
    "\n",
    "        \n",
    "        symbols = np.empty([numPackets*numSymbols,sampPerSym], dtype='complex')\n",
    "        N = symbols.shape[0]\n",
    "        k,l = 0, 0\n",
    "\n",
    "        for i in range(numPackets):\n",
    "            for j in range(numSymbols):\n",
    "                symbols[l,:] = packets[i,k:k+sampPerSym]\n",
    "                k = k + sampPerSym\n",
    "                l = l+1\n",
    "            k = 0\n",
    "\n",
    "        print(\"# of symbols extract: \" + str(symbols.shape[0]))\n",
    "        return symbols\n",
    "    else:\n",
    "        return [0]\n",
    "\n",
    "def save_data(symbols, dir_, filename):\n",
    "    if str(os.path.exists(dir_+filename)) == 'True':\n",
    "\n",
    "        with open(dir_+filename, mode='rb') as file:        \n",
    "            d = cPickle.load(file)  \n",
    "            \n",
    "        d = np.concatenate([np.asarray(d), np.asarray(symbols)])\n",
    "        \n",
    "        print('Appending to file...')\n",
    "        print('Total Symbols:' + str(d.shape))\n",
    "        cPickle.dump(d, open(dir_+filename,'wb'))\n",
    "        \n",
    "    else:\n",
    "        cPickle.dump(symbols, open(dir_+filename,'wb'))\n",
    "    print(\"Saved data to:\" + str(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = all_data_files[5]\n",
    "print(file.name)\n",
    "BW, SF, sampPerSym, L = get_and_set_encoding_params(file)\n",
    "BW, SF, sampPerSym = int(BW), int(SF), int(sampPerSym)\n",
    "print(BW, SF, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = load_data(file)\n",
    "plt.plot(np.real(np.abs((signal[1_000_000:2_000_000]))))\n",
    "\n",
    "#done: 4, 5,1,2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all data (find zero padding approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfiles = len(all_data_files)\n",
    "\n",
    "for i in trange(1):\n",
    "    file = all_data_files[5]\n",
    "    signal = load_data(file)\n",
    "    print(\"Processing:\" + str(file))\n",
    "    packets = extract_packets(file, norm_thresh=0.00015, packet_thresh=17_000)\n",
    "    \n",
    "    if len(packets) > 1:\n",
    "        BW, SF, sampPerSym, L = get_and_set_encoding_params(file)\n",
    "        BW, SF, sampPerSym = int(BW), int(SF), int(sampPerSym)\n",
    "        numPackets = np.asarray(packets).shape[0]\n",
    "\n",
    "        symbols = extract_symbols(packets, numPackets, sampPerSym)\n",
    "\n",
    "        dir_ = 'Processed_Data/Indoor/Location5/'\n",
    "        filename = \"lora_symbols_BW\" + str(BW) + \"_SF\" + str(SF) + \"_L\" + str(L) + \".p\"\n",
    "        #if len(symbols) > 2:\n",
    "        #    save_data(symbols, dir_,filename)\n",
    "            \n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.fft.fftshift(np.abs(np.fft.fft((symbols[0,:])))))\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.real(symbols[0,:]))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file.name)\n",
    "plt.plot(symbols[50,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cPickle.dump(symbols, open('lora_symbols_BW1_SF12_Att40.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of symbols per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_files2(path, pattern=r'**/*.p'):\n",
    "    yield from (file for file in data_dir.glob('**/*.p'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('Processed_Data/Indoors/Location0/')\n",
    "all_data_files = list(glob_files2(data_dir))\n",
    "\n",
    "print(\"# of files: \" + str(len(all_data_files)))\n",
    "all_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(all_data_files[12], mode='rb') as file:  \n",
    "    d1 = cPickle.load(file)  \n",
    "\n",
    "    print(file.name, d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.concatenate([d1[5,:], d1[6,:]])\n",
    "dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.fft.fftshift(np.abs(np.fft.fft((d[0,:])))))\n",
    "plt.plot(np.fft.fftshift(np.abs(np.fft.fft((dt)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
